"""
Pipeline V9.1 - Influencer Signal Analysis
Reads subtitle files, analyzes with Claude, validates, inserts to Supabase.
"""
import json, os, ssl, uuid, time, re
import urllib.request

SUBS_DIR = r"C:\Users\Mario\work\subs"
SUPABASE_URL = "https://arypzhotxflimroprmdk.supabase.co"
ANON_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImFyeXB6aG90eGZsaW1yb3BybWRrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NzIwMDYxMTAsImV4cCI6MjA4NzU4MjExMH0.qcqFIvYRiixwu609Wjj9H3HxscU8vNpo9nS_KQ3f00A"
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
SSL_CTX = ssl.create_default_context()

VALID_SIGNALS = {"ë§¤ìˆ˜", "ê¸ì •", "ì¤‘ë¦½", "ê²½ê³„", "ë§¤ë„"}
VALID_MARKETS = {"KR", "US", "US_ADR", "CRYPTO", "CRYPTO_DEFI", "SECTOR", "INDEX", "ETF", "OTHER"}
VALID_MENTION_TYPES = {"ê²°ë¡ ", "ë…¼ê±°", "ë‰´ìŠ¤", "ë¦¬í¬íŠ¸", "êµìœ¡", "í‹°ì €", "ë³´ìœ ", "ì»¨ì„¼ì„œìŠ¤", "ì„¸ë¬´", "ì°¨ìµê±°ë˜", "ì‹œë‚˜ë¦¬ì˜¤"}
VALID_CONFIDENCE = {"very_high", "high", "medium", "low", "very_low"}

# Channel mapping for sub files
CHANNEL_MAP = {
    "booread": {"name": "ë¶€ì½ë‚¨TV", "handle": "@buiknam_tv", "url": "https://www.youtube.com/@buiknam_tv"},
    "booreadman": {"name": "ë¶€ì½ë‚¨TV", "handle": "@buiknam_tv", "url": "https://www.youtube.com/@buiknam_tv"},
    "dalrant": {"name": "ë‹¬ë€íŠ¸íˆ¬ì", "handle": "@dalrant", "url": "https://www.youtube.com/@dalrant"},
    "hyoseok": {"name": "ì´íš¨ì„ì•„ì¹´ë°ë¯¸", "handle": "@hyoseok_academy", "url": "https://www.youtube.com/@hyoseok_academy"},
    "syuka": {"name": "ìŠˆì¹´ì›”ë“œ", "handle": "@syukasworld", "url": "https://www.youtube.com/@syukasworld"},
}

def supabase_request(method, table, data=None, params=""):
    url = f"{SUPABASE_URL}/rest/v1/{table}?{params}" if params else f"{SUPABASE_URL}/rest/v1/{table}"
    body = json.dumps(data).encode() if data else None
    req = urllib.request.Request(url, data=body, method=method, headers={
        "apikey": ANON_KEY,
        "Authorization": f"Bearer {ANON_KEY}",
        "Content-Type": "application/json",
        "Prefer": "return=representation",
    })
    try:
        resp = urllib.request.urlopen(req, context=SSL_CTX)
        return json.loads(resp.read()) if resp.read else []
    except urllib.error.HTTPError as e:
        err_body = e.read().decode()
        print(f"  DB Error {e.code}: {err_body[:200]}")
        return None

def supabase_get(table, params=""):
    url = f"{SUPABASE_URL}/rest/v1/{table}?{params}" if params else f"{SUPABASE_URL}/rest/v1/{table}"
    req = urllib.request.Request(url, headers={
        "apikey": ANON_KEY,
        "Authorization": f"Bearer {ANON_KEY}",
    })
    resp = urllib.request.urlopen(req, context=SSL_CTX)
    return json.loads(resp.read())

def call_claude(system_prompt, user_prompt, max_tokens=4096):
    """Call Claude API"""
    url = "https://api.anthropic.com/v1/messages"
    data = {
        "model": "claude-sonnet-4-20250514",
        "max_tokens": max_tokens,
        "system": system_prompt,
        "messages": [{"role": "user", "content": user_prompt}],
    }
    body = json.dumps(data).encode()
    req = urllib.request.Request(url, data=body, method="POST", headers={
        "x-api-key": ANTHROPIC_API_KEY,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json",
    })
    resp = urllib.request.urlopen(req, context=SSL_CTX, timeout=120)
    result = json.loads(resp.read())
    return result["content"][0]["text"]

def load_subtitle(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        d = json.load(f)
    subs = d.get('subtitles', d.get('segments', []))
    # Build transcript text with timestamps
    lines = []
    for s in subs:
        start = s.get('start', 0)
        text = s.get('text', '')
        if not text.strip() or text.strip() == '[Music]':
            continue
        mm = int(start // 60)
        ss = int(start % 60)
        lines.append(f"[{mm}:{ss:02d}] {text}")
    return "\n".join(lines), d.get('video_id'), d.get('channel'), d.get('title', '')

def build_analysis_prompt(transcript, channel_name, video_id):
    return f"""ì•„ë˜ ìœ íŠœë¸Œ ì˜ìƒì˜ ìë§‰ì„ ë¶„ì„í•´ì„œ íˆ¬ì ì‹œê·¸ë„ì„ ì¶”ì¶œí•´ì¤˜.

ì±„ë„: {channel_name}
ì˜ìƒ ID: {video_id}

## ê·œì¹™ ìš”ì•½
- ì‹œê·¸ë„ 5ë‹¨ê³„ë§Œ ì‚¬ìš©: ë§¤ìˆ˜/ê¸ì •/ì¤‘ë¦½/ê²½ê³„/ë§¤ë„
- STRONG_BUY, BUY, SELL ë“± ì˜ë¬¸ ê¸ˆì§€
- 1ì˜ìƒ 1ì¢…ëª© 1ë°œì–¸ì = ì‹œê·¸ë„ 1ê°œ
- mention_type: ê²°ë¡ /ë…¼ê±°/ë‰´ìŠ¤/ë¦¬í¬íŠ¸/êµìœ¡/í‹°ì €/ë³´ìœ /ì»¨ì„¼ì„œìŠ¤/ì„¸ë¬´/ì°¨ìµê±°ë˜/ì‹œë‚˜ë¦¬ì˜¤
- market: KR/US/US_ADR/CRYPTO/CRYPTO_DEFI/SECTOR/INDEX/ETF/OTHER
- confidence: very_high/high/medium/low/very_low
- ë…¼ê±° ì¢…ëª©ì€ ìµœëŒ€ "ê¸ì •"ê¹Œì§€. "ë§¤ìˆ˜" ê¸ˆì§€
- ë¦¬í¬íŠ¸ ì „ë‹¬(ë³¸ì¸ ì˜ê²¬ ì—†ìŒ) = ì¤‘ë¦½
- ì „ë§(~ì´ë‹¤/ëœë‹¤/ë³´ì¸ë‹¤) = ê¸ì •, ê¶Œìœ (~ì‚¬ë¼/ë‹´ì•„ë¼) = ë§¤ìˆ˜
- INDEX(ì½”ìŠ¤í”¼, S&P500 ë“±) = ì‹œê·¸ë„ ì œì™¸
- êµìœ¡/ì„¤ëª…í˜• = ì‹œê·¸ë„ ì œì™¸
- ì¡°ê±´ë¶€ ë°œì–¸ = confidence medium ì´í•˜
- ìœ ë£Œ í‹°ì € = ì‹œê·¸ë„ ìƒì„± ê¸ˆì§€
- ìŠ¤ì½”í”„: í•œêµ­ì£¼ì‹ + ë¯¸êµ­ì£¼ì‹ + í¬ë¦½í† ë§Œ (ë¶€ë™ì‚°/ì›ìì¬/í•´ì™¸ì£¼ì‹ ì œì™¸)
- ë°œì–¸ì ì´ë¦„ ì •ê·œí™” í•„ìˆ˜

## ì¶œë ¥ í˜•ì‹ (JSON)
ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ.
```json
{{
  "video_summary": "ì˜ìƒ ì „ì²´ ë‚´ìš© ìš”ì•½ (7-15ì¤„, êµ¬ì²´ì ìœ¼ë¡œ)",
  "signals": [
    {{
      "speaker": "ë°œì–¸ì ì´ë¦„",
      "stock": "ì¢…ëª©ëª…",
      "ticker": "ì¢…ëª©ì½”ë“œ (ëª¨ë¥´ë©´ null)",
      "market": "KR/US/...",
      "mention_type": "ê²°ë¡ /ë…¼ê±°/...",
      "signal": "ë§¤ìˆ˜/ê¸ì •/ì¤‘ë¦½/ê²½ê³„/ë§¤ë„",
      "confidence": "high/medium/low/...",
      "timestamp": "12:34",
      "key_quote": "í•µì‹¬ ë°œì–¸ ì›ë¬¸",
      "reasoning": "ë¶„ë¥˜ ì´ìœ "
    }}
  ]
}}
```

ì‹œê·¸ë„ì´ ì—†ìœ¼ë©´ signalsë¥¼ ë¹ˆ ë°°ì—´ë¡œ.

## ìë§‰
{transcript[:15000]}
"""

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ íˆ¬ì ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìë§‰ì„ ì½ê³  V9 íŒŒì´í”„ë¼ì¸ ê·œì¹™ì— ë”°ë¼ íˆ¬ì ì‹œê·¸ë„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡(```)ë„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ìˆœìˆ˜ JSON í…ìŠ¤íŠ¸ë§Œ."""

def validate_signals(result):
    """23-item cross validation. Returns list of issues."""
    issues = []
    signals = result.get("signals", [])
    
    for i, sig in enumerate(signals):
        prefix = f"Signal {i+1} ({sig.get('stock','?')})"
        
        # 4. 5ë‹¨ê³„ ì™¸ ë“±ê¸‰
        if sig.get("signal") not in VALID_SIGNALS:
            issues.append(f"{prefix}: Invalid signal '{sig.get('signal')}' - must be ë§¤ìˆ˜/ê¸ì •/ì¤‘ë¦½/ê²½ê³„/ë§¤ë„")
        
        # market check
        if sig.get("market") not in VALID_MARKETS:
            issues.append(f"{prefix}: Invalid market '{sig.get('market')}'")
        
        # mention_type check
        if sig.get("mention_type") not in VALID_MENTION_TYPES:
            issues.append(f"{prefix}: Invalid mention_type '{sig.get('mention_type')}'")
        
        # confidence check
        if sig.get("confidence") not in VALID_CONFIDENCE:
            issues.append(f"{prefix}: Invalid confidence '{sig.get('confidence')}'")
        
        # 1. ë…¼ê±° vs ê²°ë¡  - ë…¼ê±°ëŠ” ìµœëŒ€ ê¸ì •
        if sig.get("mention_type") == "ë…¼ê±°" and sig.get("signal") == "ë§¤ìˆ˜":
            issues.append(f"{prefix}: ë…¼ê±° ì¢…ëª©ì¸ë° ë§¤ìˆ˜ë¡œ ë¶„ë¥˜ë¨ (ìµœëŒ€ ê¸ì •)")
        
        # 3. ë‰´ìŠ¤/ë¦¬í¬íŠ¸ëŠ” ì¤‘ë¦½
        if sig.get("mention_type") in ("ë‰´ìŠ¤", "ë¦¬í¬íŠ¸") and sig.get("signal") not in ("ì¤‘ë¦½",):
            issues.append(f"{prefix}: {sig.get('mention_type')}ì¸ë° signal={sig.get('signal')} (ì¤‘ë¦½ì´ì–´ì•¼ í•¨)")
        
        # 8. INDEX ì‹œê·¸ë„ í¬í•¨ ì—¬ë¶€
        if sig.get("market") == "INDEX":
            issues.append(f"{prefix}: INDEX(ì§€ìˆ˜) ì‹œê·¸ë„ì€ ì œì™¸í•´ì•¼ í•¨")
        
        # 9. êµìœ¡ ì‹œê·¸ë„ í¬í•¨ ì—¬ë¶€
        if sig.get("mention_type") == "êµìœ¡":
            issues.append(f"{prefix}: êµìœ¡ ì½˜í…ì¸ ëŠ” ì‹œê·¸ë„ì—ì„œ ì œì™¸í•´ì•¼ í•¨")
        
        # 11. í‹°ì € ì‹œê·¸ë„
        if sig.get("mention_type") == "í‹°ì €" and sig.get("signal") != "ì¤‘ë¦½":
            issues.append(f"{prefix}: í‹°ì €ëŠ” ì¤‘ë¦½ì´ì–´ì•¼ í•¨")
        
        # timestamp check
        if not sig.get("timestamp"):
            issues.append(f"{prefix}: íƒ€ì„ìŠ¤íƒ¬í”„ ëˆ„ë½")
        
        # speaker check
        if not sig.get("speaker"):
            issues.append(f"{prefix}: ë°œì–¸ì ëˆ„ë½")
    
    # 5. ì¤‘ë³µ ì²´í¬ (ê°™ì€ ë°œì–¸ì + ì¢…ëª©)
    seen = set()
    for sig in signals:
        key = (sig.get("speaker",""), sig.get("stock",""))
        if key in seen:
            issues.append(f"ì¤‘ë³µ: {key[0]} + {key[1]}")
        seen.add(key)
    
    return issues

def get_or_create_channel(channel_info):
    """Get existing channel or create new one"""
    name = channel_info["name"]
    existing = supabase_get("influencer_channels", f"channel_name=eq.{urllib.parse.quote(name)}")
    if existing:
        return existing[0]["id"]
    
    new_ch = {
        "channel_name": name,
        "channel_handle": channel_info.get("handle", ""),
        "channel_url": channel_info.get("url", ""),
        "platform": "youtube",
    }
    result = supabase_request("POST", "influencer_channels", new_ch)
    if result:
        return result[0]["id"]
    return None

def get_or_create_video(video_id, title, channel_id, summary=""):
    """Get existing video or create new one"""
    existing = supabase_get("influencer_videos", f"video_id=eq.{video_id}")
    if existing:
        return existing[0]["id"]
    
    new_vid = {
        "video_id": video_id,
        "title": title or f"ì˜ìƒ {video_id}",
        "channel_id": channel_id,
        "has_subtitle": True,
        "subtitle_language": "ko",
        "pipeline_version": "V9.1",
    }
    result = supabase_request("POST", "influencer_videos", new_vid)
    if result:
        return result[0]["id"]
    return None

def get_or_create_speaker(name):
    """Get existing speaker or create new one"""
    import urllib.parse
    existing = supabase_get("speakers", f"name=eq.{urllib.parse.quote(name)}")
    if existing:
        return existing[0]["id"]
    
    # Check aliases
    all_speakers = supabase_get("speakers")
    for sp in all_speakers:
        if name in (sp.get("aliases") or []):
            return sp["id"]
    
    new_sp = {
        "name": name,
        "aliases": [name],
    }
    result = supabase_request("POST", "speakers", new_sp)
    if result:
        return result[0]["id"]
    return None

def insert_signal(sig, video_db_id, speaker_db_id):
    """Insert a signal into DB"""
    new_sig = {
        "video_id": video_db_id,
        "speaker_id": speaker_db_id,
        "stock": sig["stock"],
        "ticker": sig.get("ticker"),
        "market": sig["market"],
        "mention_type": sig["mention_type"],
        "signal": sig["signal"],
        "confidence": sig["confidence"],
        "timestamp": sig["timestamp"],
        "key_quote": sig.get("key_quote", ""),
        "reasoning": sig.get("reasoning", ""),
        "review_status": "pending",
        "pipeline_version": "V9.1",
    }
    result = supabase_request("POST", "influencer_signals", new_sig)
    return result

def main():
    import urllib.parse
    
    print("=" * 60)
    print("Pipeline V9.1 - Influencer Signal Analysis")
    print("=" * 60)
    
    # Get existing videos in DB
    existing_videos = supabase_get("influencer_videos")
    existing_video_ids = {v["video_id"] for v in existing_videos}
    print(f"\nExisting videos in DB: {len(existing_video_ids)}")
    
    # Get existing signals
    existing_signals = supabase_get("influencer_signals")
    existing_signal_keys = set()
    for s in existing_signals:
        existing_signal_keys.add((s["video_id"], s["speaker_id"], s["stock"]))
    print(f"Existing signals in DB: {len(existing_signals)}")
    
    # Find subtitle files to process
    # Skip files with too few segments (< 50) as they're likely incomplete
    # Skip videos already analyzed with V9+
    files_to_process = []
    for f in sorted(os.listdir(SUBS_DIR)):
        if not f.endswith('.json'):
            continue
        filepath = os.path.join(SUBS_DIR, f)
        with open(filepath, 'r', encoding='utf-8') as fh:
            d = json.load(fh)
        subs = d.get('subtitles', d.get('segments', []))
        if len(subs) < 50:
            print(f"  SKIP {f}: too few segments ({len(subs)})")
            continue
        
        vid = d.get('video_id')
        # Check if already analyzed with V9
        already_v9 = False
        for ev in existing_videos:
            if ev["video_id"] == vid and ev.get("pipeline_version","").startswith("V9"):
                already_v9 = True
                break
        
        if already_v9:
            print(f"  SKIP {f}: already analyzed with V9")
            continue
        
        prefix = f.split('_')[0]
        ch_info = CHANNEL_MAP.get(prefix)
        if not ch_info:
            print(f"  SKIP {f}: unknown channel prefix '{prefix}'")
            continue
        
        files_to_process.append((f, filepath, vid, ch_info))
    
    print(f"\nFiles to process: {len(files_to_process)}")
    for f, _, vid, ch_info in files_to_process:
        print(f"  {f} -> {ch_info['name']}")
    
    # Process each file
    all_results = []
    total_signals = 0
    
    for f, filepath, vid, ch_info in files_to_process:
        print(f"\n{'='*50}")
        print(f"Processing: {f} (video_id={vid})")
        print(f"Channel: {ch_info['name']}")
        
        transcript, _, _, title = load_subtitle(filepath)
        print(f"  Transcript length: {len(transcript)} chars")
        
        # Call Claude for analysis
        prompt = build_analysis_prompt(transcript, ch_info['name'], vid)
        print(f"  Calling Claude API...")
        
        try:
            response = call_claude(SYSTEM_PROMPT, prompt, max_tokens=4096)
        except Exception as e:
            print(f"  ERROR calling Claude: {e}")
            continue
        
        # Parse JSON response
        try:
            # Strip markdown code blocks if present
            cleaned = response.strip()
            if cleaned.startswith("```"):
                cleaned = re.sub(r'^```(?:json)?\s*', '', cleaned)
                cleaned = re.sub(r'\s*```$', '', cleaned)
            result = json.loads(cleaned)
        except json.JSONDecodeError as e:
            print(f"  ERROR parsing JSON: {e}")
            print(f"  Response preview: {response[:300]}")
            continue
        
        signals = result.get("signals", [])
        summary = result.get("video_summary", "")
        print(f"  Summary: {summary[:100]}...")
        print(f"  Signals found: {len(signals)}")
        
        # Validate
        issues = validate_signals(result)
        if issues:
            print(f"  VALIDATION ISSUES ({len(issues)}):")
            for iss in issues:
                print(f"    - {iss}")
            # Remove problematic signals
            clean_signals = []
            for sig in signals:
                has_issue = False
                if sig.get("signal") not in VALID_SIGNALS:
                    has_issue = True
                if sig.get("market") == "INDEX":
                    has_issue = True
                if sig.get("mention_type") == "êµìœ¡":
                    has_issue = True
                if not has_issue:
                    clean_signals.append(sig)
            signals = clean_signals
            print(f"  After cleanup: {len(signals)} signals")
        else:
            print(f"  Validation: PASS (0 issues)")
        
        all_results.append({
            "file": f,
            "video_id": vid,
            "channel": ch_info,
            "title": title,
            "summary": summary,
            "signals": signals,
        })
        total_signals += len(signals)
        
        # Print signals
        for sig in signals:
            print(f"  ğŸ“Š {sig['stock']} | {sig['signal']} | {sig['confidence']} | {sig['speaker']} | {sig.get('mention_type','?')}")
        
        time.sleep(1)  # Rate limit
    
    print(f"\n{'='*60}")
    print(f"ANALYSIS COMPLETE")
    print(f"Videos analyzed: {len(all_results)}")
    print(f"Total signals: {total_signals}")
    
    # Save results to file before DB insert
    results_file = os.path.join(r"C:\Users\Mario\work\invest-sns", "pipeline_v9_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"Results saved to {results_file}")
    
    # STEP 3: Insert to Supabase
    print(f"\n{'='*60}")
    print("STEP 3: Supabase INSERT")
    
    inserted_count = 0
    for res in all_results:
        vid = res["video_id"]
        ch_info = res["channel"]
        
        # Get or create channel
        channel_db_id = get_or_create_channel(ch_info)
        if not channel_db_id:
            print(f"  ERROR: Could not get/create channel {ch_info['name']}")
            continue
        
        # Get or create video
        video_db_id = get_or_create_video(vid, res["title"] or res.get("summary","")[:50], channel_db_id, res["summary"])
        if not video_db_id:
            print(f"  ERROR: Could not get/create video {vid}")
            continue
        
        for sig in res["signals"]:
            speaker_name = sig["speaker"]
            speaker_db_id = get_or_create_speaker(speaker_name)
            if not speaker_db_id:
                print(f"  ERROR: Could not get/create speaker {speaker_name}")
                continue
            
            # Check duplicate
            dup_key = (video_db_id, speaker_db_id, sig["stock"])
            if dup_key in existing_signal_keys:
                print(f"  SKIP duplicate: {speaker_name} + {sig['stock']}")
                continue
            
            result = insert_signal(sig, video_db_id, speaker_db_id)
            if result:
                inserted_count += 1
                existing_signal_keys.add(dup_key)
                print(f"  âœ… Inserted: {sig['stock']} | {sig['signal']} | {speaker_name}")
            else:
                print(f"  âŒ Failed: {sig['stock']} | {speaker_name}")
    
    print(f"\n{'='*60}")
    print(f"FINAL REPORT")
    print(f"  Videos analyzed: {len(all_results)}")
    print(f"  Total signals extracted: {total_signals}")
    print(f"  Signals inserted to DB: {inserted_count}")
    print(f"  Pipeline version: V9.1")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
